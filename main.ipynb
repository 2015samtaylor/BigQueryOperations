{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#SFTP conn\n",
    "import pysftp\n",
    "import json\n",
    "cnopts = pysftp.CnOpts()\n",
    "cnopts.hostkeys = None\n",
    "\n",
    "\n",
    "with open('powerschool-420113-b0750ef59b13.json') as json_file:\n",
    "    j = json.load(json_file) \n",
    "    sftp_pass = j['sftp_password']\n",
    "\n",
    "#Get Data from PS Data Export Manager SFTP folder, and move to BigQuery\n",
    "with pysftp.Connection(\n",
    "    host=\"sftp.iotaschools.org\",\n",
    "    username=\"iota.sftp\",\n",
    "    password=sftp_pass,\n",
    "    cnopts=cnopts\n",
    ") as sftp:\n",
    "    \n",
    "    directory_contents = sftp.listdir()\n",
    "    current_directory = sftp.pwd\n",
    "    print(\"Current directory:\", current_directory)\n",
    "    print('Dir contents: ', directory_contents)\n",
    "\n",
    "    # Download a remote file to the local machine\n",
    "    # remote_directory = \"/greendottn/custom_report_standards_2024\"\n",
    "    # for item in directory_contents:\n",
    "        # print(item)\n",
    "\n",
    "    local_file = \"local_file.txt\"   #(This can be dynamic if we want to preserve the file everytime)\n",
    "    sftp.get('Test!.txt', local_file)\n",
    "    sftp.close()\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "from modules.buckets import create_bucket, upload_to_bucket, download_from_bucket, upload_to_bq_table\n",
    "import os\n",
    "import pandas_gbq\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Set the GOOGLE_APPLICATION_CREDENTIALS environment variable in order to interact\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = 'powerschool-420113-b0750ef59b13.json'\n",
    "\n",
    "\n",
    "\n",
    "class Create:\n",
    "\n",
    "    def __init__(self, bucket, end_file_name, local_file_name, project_id, db, table_name, location=None):\n",
    "        \n",
    "        self.location = location\n",
    "        self.bucket = bucket\n",
    "        self.end_file_name = end_file_name\n",
    "        self.local_file_name = local_file_name\n",
    "        self.project_id = project_id\n",
    "        self.db = db\n",
    "        self.table_name = table_name\n",
    "\n",
    "\n",
    "    def process(self):\n",
    "    \n",
    "        #Create the bucket, and upload to that bucket. If already created, bypass\n",
    "        # create_bucket(self.bucket, self.location)\n",
    "\n",
    "        #Upload file to bucket, demonstrates if overwritten or newfile. \n",
    "        #End File Name,  Local File Path, Bucket Name\n",
    "        # upload_to_bucket(self.end_file_name , self.local_file_name, self.bucket)\n",
    "\n",
    "\n",
    "        upload_to_bq_table(cloud_storage_uri = f'gs://{self.bucket}/{self.end_file_name}',\n",
    "                        project_id = self.project_id,\n",
    "                        db = self.db,\n",
    "                        table_name = self.table_name,\n",
    "                        location = self.location)\n",
    "        \n",
    "        SQL_query = f'''\n",
    "            SELECT * FROM {self.project_id}.{self.db}.{self.table_name}\n",
    "        '''\n",
    "\n",
    "        try:\n",
    "            print('Running query')\n",
    "            query = pandas_gbq.read_gbq(SQL_query, project_id=self.project_id, location=self.location)\n",
    "            print('Query completed')\n",
    "        except:\n",
    "            print('Unable to run query')\n",
    "\n",
    "\n",
    "        return(query)\n",
    "\n",
    "\n",
    "# SELECT * FROM `greendotdataflow.Students.Student_Records_10` LIMIT 1000\n",
    "# SELECT * FROM `powerschool-420113.students_1.students` LIMIT 1000\n",
    "\n",
    "# project_id = 'powerschool-420113'\n",
    "# db = 'students_1'\n",
    "# table_name = 'students'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "already_created_update = Create(location = 'us-south1',\n",
    "                                bucket='psholdingbucket7',\n",
    "                                end_file_name = 'students.csv',\n",
    "                                local_file_name = 'Student_Records.csv',\n",
    "                                project_id='powerschool-420113',\n",
    "                                db = 'students_1',\n",
    "                                table_name = 'students'\n",
    "                    )\n",
    "\n",
    "# brand_new_creation = Create(location = 'us-south1',\n",
    "#                             bucket='psholdingbucket12',\n",
    "#                             end_file_name = 'students.csv',\n",
    "#                             local_file_name = 'Student_Records.csv',\n",
    "#                             project_id='greendotdataflow',\n",
    "#                             db = 'Students',\n",
    "#                             table_name = 'Student_Records_12',\n",
    "\n",
    "#                             SQL_query='''\n",
    "\n",
    "#                             SELECT * FROM greendotdataflow.Students.Student_Records_12\n",
    "\n",
    "#                             '''  \n",
    "#                     )\n",
    "\n",
    "\n",
    "query_one = already_created_update.process()\n",
    "# query_two = brand_new_creation.process()\n",
    "\n",
    "#-------------Next step put file in clevers SFTP--------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
